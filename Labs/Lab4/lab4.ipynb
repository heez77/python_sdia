{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 4 - K-nearest neighbours (K-NN) classification with numpy, scikit-learn, cython and numba\n",
    "\n",
    "Students (pair):\n",
    "- [Jérémy Jean](https://github.com/heez77)\n",
    "- [Maxime Gey](https://github.com/Purjack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful references for this lab**:\n",
    "\n",
    "[1] scikit-learn: [documentation](https://scikit-learn.org/stable/modules/neighbors.html?highlight=knn%20classification)\n",
    "\n",
    "[2] `numba`: [documentation](http://numba.pydata.org/) \n",
    "\n",
    "[3] cython: [a very useful tutorial](https://cython.readthedocs.io/en/latest/src/userguide/numpy_tutorial.html#numpy-tutorial), and [another one](http://docs.cython.org/en/latest/src/tutorial/cython_tutorial.html)\n",
    "\n",
    "\n",
    "\n",
    "## <a name=\"content\">Contents</a>\n",
    "- [Exercise 1: KNN classification with numpy and sklearn](#ex1)\n",
    "- [Exercise 2: Code acceleration with cython](#ex2)\n",
    "- [Exercise 3: Code acceleration with numba](#ex3)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ex1\">Exercise 1: K-Nearest Neighbours (K-NN) classification with numpy and scikit-learn</a> [(&#8593;)](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session is a first introduction to classification using the most intuitive non parametric method: the $K$-nearest neighbours. The principle is [the following](https://scikit-learn.org/stable/modules/neighbors.html?highlight=knn%20classification). A set of labelled observations is given as a learning set. A classification taks then consists in assigning a label to any new observation. In particular, the K-NN approach consists in assigning to the observation the most frequent label among its $K$ nearest neighbours taken in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Validation on synthetic data\n",
    "\n",
    "Load the training and test datasets `data/synth_train.txt` and `data/synth_test.txt`. Targets belong to the set $\\{1,2\\}$ and entries belong to $\\mathbb{R}^2$. The file `data/synth_train.txt` contain 100 training data samples, and `data/synth_test.txt` contains 200 test samples, where:\n",
    "\n",
    "- the 1st column contains the label of the class the sample;\n",
    "- columns 2 & 3 contain the coordinates of each sample (in $\\mathbb{R}^2$).\n",
    "\n",
    "Useful commands can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# load the training set\n",
    "train = np.loadtxt('data/synth_train.txt')  #...,delimiter=',') if there are ',' as delimiters\n",
    "class_train = train[:,0]\n",
    "x_train = train[:,1:]\n",
    "N_train = train.shape[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# load the test set\n",
    "test = np.loadtxt('/datasynth_test.txt') \n",
    "class_test_1 = test[test[:,0]==1]\n",
    "class_test_2 = test[test[:,0]==2]\n",
    "x_test = test[:,1:]\n",
    "N_test = test.shape[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Display the training set and distinguish the two classes. \n",
    "\n",
    "> Hint: useful functions include `matplotlib.pyplot.scatter` or `matplotlib.pyplot.plot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = np.loadtxt('data/synth_train.txt')  \n",
    "df_train = pd.DataFrame(train, columns = ['classe', 'x1', 'x2'])\n",
    "N_train = df_train.size\n",
    "\n",
    "test = np.loadtxt('data/synth_test.txt') \n",
    "df_test = pd.DataFrame(test, columns = ['classe', 'x1', 'x2'])\n",
    "N_test = df_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df_train,x='x1',y='x2',color='classe',title='Data visualization of the training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df_test,x='x1',y='x2',color='classe',title='Data visualization of the test dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement the K-nearest neighbours algorithm for classification.\n",
    "\n",
    "Hint:\n",
    "\n",
    "- useful functions include numpy.linalg.norm, numpy.argsort, numpy.bincount;\n",
    "- implement the algorithm as a function rather than an object. This will drastically simplify the acceleration step using Cython.\n",
    "- for an optimized partial sorting procedure, you may have a look at the bottleneck.argpartition function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_nearest_neighbors():\n",
    "    \"\"\"K nearest neighbors implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, train:np.ndarray, test:np.ndarray, K:int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (np.ndarray): The training dataset\n",
    "            test (np.ndarray): The test dataset\n",
    "            K (int): K value in the nearest neighbors implementation, must be strictly positive.\n",
    "        \"\"\"\n",
    "        self.x_train = train[:,1:]\n",
    "        self.y_train = train[:,0]\n",
    "        self.N_train = train.shape[0]\n",
    "        self.x_test = test[:,1:]\n",
    "        self.y_test = test[:,0]\n",
    "        self.N_test = test.shape[0]\n",
    "        assert K>0, \"K must be strictly positive\"\n",
    "        self.K = K\n",
    "\n",
    "    def classifier(self, x_new:np.ndarray)->float:\n",
    "        \"\"\"Compute the classification using K nearest neighbors algorithm method.\n",
    "\n",
    "        Args:\n",
    "            x_new (np.ndarray): The input to be predicted.\n",
    "\n",
    "        Returns:\n",
    "            float: 1. if x_new is in class 1 else 2.\n",
    "        \"\"\"\n",
    "        distances = []\n",
    "        counter1 = 0\n",
    "        counter2 = 0\n",
    "        for i in range(self.N_train):\n",
    "            distances.append((np.linalg.norm(x_new-self.x_train[i]),self.y_train[i]))\n",
    "        dtype = [('distance', float), ('target', float)]\n",
    "        distances=np.array(distances,dtype=dtype)\n",
    "        distances=np.sort(distances,order='distance')  # plus petites en premier\n",
    "        for j in range(0,self.K):\n",
    "            if distances[j][1]==1.:\n",
    "                counter1+=1\n",
    "            if distances[j][1]==2.:\n",
    "                counter2+=1\n",
    "            if counter1+counter2==self.K:\n",
    "                break\n",
    "        if counter1>counter2:\n",
    "            return 1.\n",
    "        else:\n",
    "            return 2.\n",
    "\n",
    "    def predict(self)->np.ndarray:\n",
    "        \"\"\"Compute the prediction for the test dataset.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array in dimension 1 with all the predictions.\n",
    "        \"\"\"\n",
    "        predictions = np.zeros(self.N_test, dtype=float)\n",
    "        for i in range(self.N_test):\n",
    "            predictions[i] = self.classifier(self.x_test[i])\n",
    "        return predictions\n",
    "\n",
    "    def error_rate(self)->float:\n",
    "        \"\"\"Compute the error rate for the test dataset.\n",
    "\n",
    "        Returns:\n",
    "            float: Error rate value, a float between 0 and 1.\n",
    "        \"\"\"\n",
    "        predictions = self.predict()\n",
    "        error_rate = 1-np.count_nonzero(predictions==self.y_test)/self.N_test\n",
    "        return error_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Compute the error rate on the training set and the test set for $K \\in \\{1,2, \\dotsc, 20\\}$. Display the classification result (see 1.) for the configuration with the lowest error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate_analyse(min:int,max:int, train:np.ndarray, test:np.ndarray)->pd.DataFrame:\n",
    "    \"\"\"Make an analysis of the error rate for a train and test dataset with different values of K. Compute the error rate for K values between min and max with a step of 1.\n",
    "\n",
    "    Args:\n",
    "        min (int): The min value for K in the K nearest neighbors algorithm.\n",
    "        max (int): The max value for K in the K nearest neighbors algorithm.\n",
    "        train (np.ndarray): Training dataset.\n",
    "        test (np.ndarray): Test dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Returns a DataFrame with two columns, one for the K value and a second for the error rate.\n",
    "    \"\"\"\n",
    "    K_values = np.arange(min,max+1)\n",
    "    error_rate = np.zeros(max-min+1,dtype=float)\n",
    "    for i,K in enumerate(K_values):\n",
    "        Knn_classifier = K_nearest_neighbors(train, test, K)\n",
    "        error_rate[i] = Knn_classifier.error_rate()\n",
    "    return pd.DataFrame(dict(K=K_values,error_rate=error_rate))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyse_error_rate = error_rate_analyse(1,20,train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_analyse_error_rate,x='K', y='error_rate', title='Evolution of the error rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best K value is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain this classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knn_classifier = K_nearest_neighbors(train, test, 3)\n",
    "predictions = Knn_classifier.predict()\n",
    "df_predictions = df_test.copy()\n",
    "df_predictions['predictions'] = predictions\n",
    "px.scatter(df_predictions ,x='x1',y='x2',color='predictions', title='Visualization of the classification with K=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Compare the results of you implementation with those of [`sklearn.neighbors.KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier). Compare the runtime of these two versions using the [`timeit`](https://docs.python.org/3/library/timeit.html) module (see session 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "Knn_sklearn = KNeighborsClassifier(3)\n",
    "Knn_sklearn.fit(train[:,1:], train[:,0])\n",
    "sklearn_predictions = Knn_sklearn.predict(test[:,1:])\n",
    "error_rate_sklearn = 1-np.count_nonzero(predictions==test[:,0])/test.shape[0]\n",
    "print(error_rate_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same error rate which is logical because the algorithm is deterministic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "loop = 10\n",
    "execution_time = timeit.timeit(lambda: Knn_classifier.predict(), number=loop) / loop\n",
    "\n",
    "execution_time_sklearn = timeit.timeit(lambda: Knn_sklearn.fit(train[:,1:], train[:,0]), number=loop) / loop + timeit.timeit(lambda: Knn_sklearn.predict(test[:,1:]), number=loop) / loop\n",
    "\n",
    "print(execution_time/execution_time_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution time of the implementation by sklearn is much faster than ours (factor 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Application to a real dataset (Breast cancer Wisconsin).\n",
    "\n",
    "6\\. Apply the K-NN classifier to the real dataset `data/wdbc12.data.txt.` Further details about the data are provided in `data/wdbc12.names.txt`.\n",
    "\n",
    "> Hint: you can use the function [`train_test_split` from `sklearn.model_selection`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the dataset into a training and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.loadtxt('data/wdbc12.data.txt', delimiter=',')[:,1:]\n",
    "train_cancer, test_cancer = train_test_split(data, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyse_error_rate = error_rate_analyse(1,20,train_cancer, test_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_analyse_error_rate,x='K', y='error_rate', title='Evolution of the error rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value of K is 11. We obtain an error rate of value 0.049."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ex2\">Exercise 2: Code acceleration with cython</a> [(&#8593;)](#content)\n",
    "\n",
    "Cython allows C code to be easily interfaced with Python. It can be useful to make your code faster for a small coding effort, in particular when using loops. A general approach to optimize your code is outlined in the [Scipy lecture notes, Section 2.4](https://scipy-lectures.org/advanced/optimizing/index.html). Complementary reading about interfacing Python with C can be found in [Section 2.8](https://scipy-lectures.org/advanced/interfacing_with_c/interfacing_with_c.html).\n",
    "\n",
    "1\\. Read carefully the [cython tutorial](http://docs.cython.org/en/latest/src/tutorial/cython_tutorial.html), which describes step by the step how the toy example reported below has been developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**: Compile the toy example provided in `example_cy/` by running, in the command line (anaconda prompt on windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd example_cy && python setup.py build_ext --inplace\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the compilation process has been slightly automatised with the instructions reported in `example_cy/setup.py`. To test the module, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd example_cy && python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import example_cy.helloworld as toy\n",
    "\n",
    "toy.printhello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which should display\n",
    "```python\n",
    "Hello World\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: \n",
    "> - do not forget to include an empty `__init__.py` file in the directory where your source code lives (`import` will fail if this is not the case).\n",
    "> - in case you have any setup issue, take a look at the `notes.md` file.\n",
    "> - if the C code and/or the executable do not seem to be regenerated by the build instructions, delete the C code and the executable first, and re-execute the compilation afterwards.\n",
    "> - do not hesitate to restart the Python kernel if necessary when the Cython executable has been re-generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Read the [Numpy/Cython tutorial](https://cython.readthedocs.io/en/latest/src/userguide/numpy_tutorial.html#numpy-tutorial), focussing on the paragraphs **Cython at a glance**, and **Your Cython environment** until **\"More generic code\"**. An example to compile a `.pyx` file depending on `numpy` is included in `example_np_cy/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks: \n",
    "> - the `annotate=True` flag in the `setup.py` allows an additional `.html` document to be generated (`<your_module_name>.html`), showing, for each line of the Cython code, the associated C instructions generated. Highlighted in yellow are the interactions with Python: the darker a region appears, the less efficient the generated C code is for this section. Work in priority on these! \n",
    "> - make sure all the previously generated files are deleted to allow the .html report to be generated;\n",
    "> - if you are working on your own machine and don't have a C/C++ compiler installed, read the notes provided in `notes.md`;\n",
    "> - use `cdef` for pure C functions (not exported to Python), `cpdef` should be favored for functions containing C instructions and later called from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd example_np_cy && python setup.py build_ext --inplace\n",
    "\n",
    "array_1 = np.random.uniform(0, 1000, size=(3000, 2000)).astype(np.intc)\n",
    "array_2 = np.random.uniform(0, 1000, size=(3000, 2000)).astype(np.intc)\n",
    "\n",
    "a = 4\n",
    "b = 3\n",
    "c = 9\n",
    "\n",
    "from example_np_cy.compute_cy import compute\n",
    "compute(array_1, array_2, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_without_cython():\n",
    "    from example_np_cy.compute import clip, compute\n",
    "    import numpy as np\n",
    "    array_1 = np.random.uniform(0, 1000, size=(3000, 2000)).astype(np.intc)\n",
    "    array_2 = np.random.uniform(0, 1000, size=(3000, 2000)).astype(np.intc)\n",
    "\n",
    "    a = 4\n",
    "    b = 3\n",
    "    c = 9\n",
    "    compute(array_1, array_2, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_with_cython():\n",
    "    %%cython\n",
    "    from example_np_cy.compute import clip, compute\n",
    "    import numpy as np\n",
    "    \n",
    "    array_1 = np.random.uniform(0, 1000, size=(3000, 2000)).astype(np.intc)\n",
    "    array_2 = np.random.uniform(0, 1000, size=(3000, 2000)).astype(np.intc)\n",
    "\n",
    "    a = 4\n",
    "    b = 3\n",
    "    c = 9\n",
    "    compute(array_1, array_2, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "loop = 10\n",
    "execution_time = timeit.timeit(lambda: execution_without_cython, number=loop) / loop\n",
    "\n",
    "execution_time_cython = timeit.timeit(lambda: execution_with_cython, number=loop) / loop\n",
    "\n",
    "print(execution_time/execution_time_cython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <i>compute</i> function is already computed using numpy and therfore really fast. It seems overkill to use Cython in this case buut we still notice a 1.27 factor in terms of rapidity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Use Cython to implement a faster version of the numpy K-NN classifier implemented in [Exercise 1](#ex1). To do so, apply step-by-step the techniques introduced in the [Numpy/Cython tutorial](https://cython.readthedocs.io/en/latest/src/userguide/numpy_tutorial.html#numpy-tutorial) (*i.e.*, compile and time your code after each step to report the evolution, keeping track of the different versions of the cython function).\n",
    "\n",
    "> Hint: if you keep numpy arrays, make sure you use memory views (see numpy/cython tutorial) to access the elements within it. Be extremely careful with the type of the input arrays (you may need to recast the format of the input elements before entering the function. The `numpy.asarray` function can prove useful).\n",
    "\n",
    "> **Detailed guidelines**: a few notes and *caveat* to help you re-writing your code in cython:\n",
    "> - try to reduce the number of calls to numpy instructions as much as possible;\n",
    "> - **you do not have to optimize everything**. For the KNN function above, most of the time is spent in computing euclidean distances: you can thus focus on optimizing tihs operations by explicitly writing a for loop, which will ensure a minimal interaction with numpy when generating the associated C code at compilation. Calls to other numpy functions can be kept as-is;\n",
    "> - if you need to create an array within the cython function, used np.zeros (**do NOT use python lists**), and use a memory view to access its content;\n",
    "> - specify the type for all variables and numpy arrays. Pay attention to the type of the input arrays passed to the Cython function;\n",
    "> - whenever an array is returned, use memory views and index(es) to efficiently access its content;\n",
    "> - some numpy operators (e.g., broadcasting mechanism) do not work with memory views. In this case, you can directly write for loop(s) to encode the operation of interest (the loops will be optimized out at compile time);\n",
    "> - only use at the final development stage the following cython optimization (not before, as they can crash the program without any help):\n",
    ">\n",
    ">```python\n",
    ">@cython.boundscheck(False)\n",
    ">@cython.wraparound(False)\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd knn && python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from knn.cython_metric import mydist\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "  return np.sum((y-x)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_cython():\n",
    "    classifier_cython = KNeighborsClassifier(3, metric=mydist)\n",
    "\n",
    "    t1 = time.time()\n",
    "    classifier_cython.fit(train[:,1:], train[:,0])\n",
    "\n",
    "    res = classifier_cython.predict(test[:,1:])\n",
    "    t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution():\n",
    "    classifier = KNeighborsClassifier(3, metric=dist)\n",
    "\n",
    "    t1 = time.time()\n",
    "    classifier.fit(train[:,1:], train[:,0])\n",
    "\n",
    "    res = classifier.predict(test[:,1:])\n",
    "    t2 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Compare the runtime of the two algorithms (using `timeit.timeit`), and conclude about the interest of using cython in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "loop = 10\n",
    "execution_time = timeit.timeit(lambda: execution_cython, number=loop) / loop\n",
    "\n",
    "execution_time_cython = timeit.timeit(lambda: execution, number=loop) / loop\n",
    "\n",
    "print(execution_time/execution_time_cython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a KNN very optimized by the sklearn library, we see that Cython allows us to compute two times faster !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ex3\">Exercise 3: Code acceleration with numba</a> [(&#8593;)](#content)\n",
    "\n",
    "`numba` is a just-in-time (JIT) compiler which translates Python codes into efficient machine code at runtime. A significant acceleration can be obtained by adding a few simple decorators to a standard Python function, up to a few restrictions detailed [here](http://numba.pydata.org/numba-doc/latest/user/performance-tips.html).\n",
    "\n",
    "If you have written most of the KNN classifier of exercise 1 with numpy, there is little to no chance that you will get an acceleration with numba (justifying the use of cython in this case). An interesting acceleration factor can however be obtained for the computation of the total variation investigated in session 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Take a look at the [numba 5 min tour](http://numba.pydata.org/numba-doc/latest/user/5minguide.html), and accelerate the total variation code from session 2 with the `@jit` decorator. You may have to rewrite small portions of your code to get the expected acceleration (see [performance tips](http://numba.pydata.org/numba-doc/latest/user/performance-tips.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison for Knn : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "\n",
    "### We put the predict and classifier functions in one function\n",
    "@njit\n",
    "def predict_K_nearest_neighbors_with_numba(x_train:np.ndarray,y_train:np.ndarray, x_test:np.ndarray, K:int):\n",
    "    N_test = test.shape[0]\n",
    "    N_train = x_train.shape[0]\n",
    "    predictions = np.zeros(N_test, dtype=float)\n",
    "    for i in range(N_test):   \n",
    "        distances = []\n",
    "        targets = []\n",
    "        counter1 = 0\n",
    "        counter2 = 0\n",
    "        for j in range(N_train):\n",
    "            # separation in two different list to help numba\n",
    "            distances.append(np.linalg.norm(x_test[i]-x_train[j]))\n",
    "            targets.append(y_train[j])\n",
    "        distances=np.array(distances,dtype=np.float64)\n",
    "        idx_sort = np.argsort(distances)\n",
    "        for k in range(0,K):\n",
    "            if targets[idx_sort[k]]==1.:\n",
    "                counter1+=1\n",
    "            if targets[idx_sort[k]]==2.:\n",
    "                counter2+=1\n",
    "            if counter1+counter2==K:\n",
    "                break\n",
    "        if counter1>counter2:\n",
    "            predictions[i] = 1.\n",
    "        else:\n",
    "            predictions[i] = 2.\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_K_nearest_neighbors(x_train:np.ndarray,y_train:np.ndarray, x_test:np.ndarray, K:int):\n",
    "    N_test = test.shape[0]\n",
    "    N_train = x_train.shape[0]\n",
    "    predictions = np.zeros(N_test, dtype=float)\n",
    "    for i in range(N_test):   \n",
    "        distances = []\n",
    "        targets = []\n",
    "        counter1 = 0\n",
    "        counter2 = 0\n",
    "        for j in range(N_train):\n",
    "            # separation in two different list to help numba\n",
    "            distances.append(np.linalg.norm(x_test[i]-x_train[j]))\n",
    "            targets.append(y_train[j])\n",
    "        distances=np.array(distances,dtype=np.float64)\n",
    "        idx_sort = np.argsort(distances)\n",
    "        for k in range(0,K):\n",
    "            if targets[idx_sort[k]]==1.:\n",
    "                counter1+=1\n",
    "            if targets[idx_sort[k]]==2.:\n",
    "                counter2+=1\n",
    "            if counter1+counter2==K:\n",
    "                break\n",
    "        if counter1>counter2:\n",
    "            predictions[i] = 1.\n",
    "        else:\n",
    "            predictions[i] = 2.\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = predict_K_nearest_neighbors_with_numba(train[:,1:], train[:,0], test[:,1:], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = 10\n",
    "execution_time = timeit.timeit(lambda: predict_K_nearest_neighbors(train[:,1:], train[:,0], test[:,1:], 3), number=loop) / loop\n",
    "\n",
    "execution_time_numba = timeit.timeit(lambda: predict_K_nearest_neighbors_with_numba(train[:,1:], train[:,0], test[:,1:], 3), number=loop) / loop\n",
    "\n",
    "print(execution_time/execution_time_numba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is 10 time faster with numba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison for Total variation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "def gradient2D(X:np.array)->tuple:\n",
    "    \"\"\"This function computes the 2D discrete gradient operator D applied to a matrix X of dimensions 2\n",
    "\n",
    "    Args:\n",
    "        X (array): A matrix in C^(M,N)\n",
    "\n",
    "    Returns:\n",
    "        (array,array) : A tuple in C^(M,N) x C^(M,N)\n",
    "    \"\"\"\n",
    "    assert X.ndim <=2, \"The input array has more than 2 dimensions\"\n",
    "\n",
    "    XDh = np.zeros(X.shape)\n",
    "    for n in range(1,XDh.shape[1]):\n",
    "        XDh[:,n-1] = X[:,n]-X[:,n-1]\n",
    "\n",
    "    DvX = np.zeros(X.shape)\n",
    "    for m in range(1,XDh.shape[0]):\n",
    "        DvX[m-1,:] = X[m,:]-X[m-1,:]\n",
    "\n",
    "\n",
    "    return XDh, DvX\n",
    "\n",
    "def tv(X:np.array)->float:\n",
    "    \"\"\"This function compute the discrete isotropic total variation of an input matrix in C^(M,N)\n",
    "\n",
    "    Args:\n",
    "        X (np.array): A matrix in C^(MxN)\n",
    "\n",
    "    Returns:\n",
    "        float: returns the value of the TV for the input matrix X\n",
    "    \"\"\"\n",
    "    XDh, DvX = gradient2D(X)\n",
    "    sum = 0\n",
    "    for m in range(XDh.shape[0]):\n",
    "        for n in range(XDh.shape[1]):\n",
    "            sum+= np.sqrt(XDh[m,n]**2 + DvX[m,n]**2)\n",
    "    return sum\n",
    "\n",
    "\n",
    "@njit\n",
    "def gradient2D_numba(X:np.array)->tuple:\n",
    "    \"\"\"This function computes the 2D discrete gradient operator D applied to a matrix X of dimensions 2\n",
    "\n",
    "    Args:\n",
    "        X (array): A matrix in C^(M,N)\n",
    "\n",
    "    Returns:\n",
    "        (array,array) : A tuple in C^(M,N) x C^(M,N)\n",
    "    \"\"\"\n",
    "    assert X.ndim <=2, \"The input array has more than 2 dimensions\"\n",
    "\n",
    "    XDh = np.zeros(X.shape)\n",
    "    for n in range(1,XDh.shape[1]):\n",
    "        XDh[:,n-1] = X[:,n]-X[:,n-1]\n",
    "\n",
    "    DvX = np.zeros(X.shape)\n",
    "    for m in range(1,XDh.shape[0]):\n",
    "        DvX[m-1,:] = X[m,:]-X[m-1,:]\n",
    "\n",
    "\n",
    "    return XDh, DvX\n",
    "\n",
    "@njit\n",
    "def tv_numba(X:np.array)->float:\n",
    "    \"\"\"This function compute the discrete isotropic total variation of an input matrix in C^(M,N)\n",
    "\n",
    "    Args:\n",
    "        X (np.array): A matrix in C^(MxN)\n",
    "\n",
    "    Returns:\n",
    "        float: returns the value of the TV for the input matrix X\n",
    "    \"\"\"\n",
    "    XDh, DvX = gradient2D_numba(X)\n",
    "    sum = 0\n",
    "    for m in range(XDh.shape[0]):\n",
    "        for n in range(XDh.shape[1]):\n",
    "            sum+= np.sqrt(XDh[m,n]**2 + DvX[m,n]**2)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Compare the runtime of the your numpy implementation and the `numba`-accelerated version (using `timeit.timeit`). \n",
    "> **Warning**: first run the numba version once to trigger the compilation, and then time it as usual. This is needed to avoid including the JIT compilation step in the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = 100\n",
    "rng = np.random.default_rng(84548)\n",
    "M,N = rng.integers(low=50, high=100, size = 2)\n",
    "X = rng.random((M,N))\n",
    "tv_numba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = 100\n",
    "rng = np.random.default_rng(84548)\n",
    "M,N = rng.integers(low=50, high=100, size = 2)\n",
    "X = rng.random((M,N))\n",
    "\n",
    "execution_time = timeit.timeit(lambda: tv(X), number=loop) / loop\n",
    "\n",
    "execution_time_numba = timeit.timeit(lambda: tv_numba(X), number=loop) / loop\n",
    "\n",
    "print(execution_time/execution_time_numba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation with numba is 230 time faster which is really better than for Knn classifier. For this implementation we really see the advantages of numba."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "3e86c5fcaeb7504a0c486c54f5e7f20bce8324b88f64f392f8b6244d9f0e8929"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
