{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 5 - Parallel Markov chains with multiprocessing and dask\n",
    "\n",
    "Students (pair):\n",
    "- [Jérémy Jean](https://github.com/heez77)\n",
    "- [Maxime Gey](https://github.com/Purjack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful references for this lab**:\n",
    "\n",
    "[1] `seaborn`: [official tutorial](https://seaborn.pydata.org/tutorial.html)\n",
    "\n",
    "[2] `multiprocessing`: [documentation](https://docs.python.org/3/library/multiprocessing.html), [doc2](https://he-arc.github.io/livre-python/multiprocessing/index.html)\n",
    "\n",
    "[3] `dask`: [documentation](http://numba.pydata.org/) \n",
    "\n",
    "## <a name=\"content\">Contents</a>\n",
    "- [Exercise 1: seaborn, a useful tool for data visualisation](#ex1)\n",
    "- [Exercise 2: Simulating a discrete-time homogeneous Markov chain](#ex2)\n",
    "- [Bonus: Parallel computing with Dask](#bonus)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ex1\">Exercise 1: seaborn, a useful tool for data visualisation</a> [(&#8593;)](#content)\n",
    " \n",
    "The `seaborn` package can significantly enhance data and data analysis visualization. See the [tutorial page](https://seaborn.pydata.org/tutorial.html) for examples of effective predefined graphics. An example aimed at visualizing the empirical distributions of 9 realizations of a bivariate Gaussian random vector is reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"dark\")\n",
    "rng = np.random.default_rng(50)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True)\n",
    "\n",
    "# Rotate the starting point around the cubehelix hue circle\n",
    "for ax, s in zip(axes.flat, np.linspace(0, 3, 10)):\n",
    "\n",
    "    # Create a cubehelix colormap to use with kdeplot\n",
    "    cmap = sns.cubehelix_palette(start=s, light=1, as_cmap=True)\n",
    "\n",
    "    # Generate and plot a random bivariate dataset\n",
    "    x, y = rng.normal(size=(2, 50))\n",
    "    sns.kdeplot(x=x, y=y, cmap=cmap, shade=True, cut=5, ax=ax)\n",
    "    ax.set(xlim=(-3, 3), ylim=(-3, 3))\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Comment on the lines of codes related to the `seaborn` library to make their role explicit. More specifically comment on the KDE method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The set method is an alias for the set_theme method which set aspects of the visual theme for all matplotlib and seaborn plots\n",
    "- The cubehelix_palette method makes a sequential palette from the cubehelix system. This function produces a colormap with linearly-decreasing (or increasing) brightness. That means that information will be preserved if printed to black and white or viewed by someone who is colorblind. “cubehelix” is also available as a matplotlib-based palette, but this function gives the user more control over the look of the palette and has a different set of defaults.\n",
    "- The kdeplot method plots univariate or bivariate distributions using kernel density estimation. A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions. Relative to a histogram, KDE can produce a plot that is less cluttered and more interpretable, especially when drawing multiple distributions. But it has the potential to introduce distortions if the underlying distribution is bounded or not smooth. Like a histogram, the quality of the representation also depends on the selection of good smoothing parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For one of the realizations, take a look at the documentation of [`sns.jointplot`](https://seaborn.pydata.org/examples/joint_kde.html) to display both the 2-D empirical distribution of the data, and 1D histograms of their distribution along each axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"dark\")\n",
    "rng = np.random.default_rng(50)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, axes = plt.subplots(3, 3, figsize=(9, 9), sharex=True, sharey=True)\n",
    "\n",
    "# Rotate the starting point around the cubehelix hue circle\n",
    "for ax, s in zip(axes.flat, np.linspace(0, 3, 10)):\n",
    "\n",
    "    # Generate and plot a random bivariate dataset\n",
    "    x, y = rng.normal(size=(2, 50))\n",
    "    sns.jointplot(x=x, y=y, cmap=cmap)\n",
    "    ax.set(xlim=(-3, 3), ylim=(-3, 3))\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ex2\">Exercise 2: Simulating a discrete-time homogeneous Markov chain.</a> [(&#8593;)](#content)\n",
    "\n",
    "\n",
    "Let ${(X_n)}_{n\\geq 0}$ be a discrete-time homogeneous Markov chain with values over a finite ensemble $E=\\{x_1,\\dots,x_N\\}$ identified to $\\{1,\\dots,N\\}$. Consider $\\boldsymbol{\\rho} \\in \\Delta_N$, where $\\Delta_N = \\{\\mathbf{x}\\in\\mathbb{R}^N \\mid x_n \\geq 0 \\, \\forall n \\in \\{1,\\dotsc,N\\} \\text{ and } \\sum_n x_n = 1 \\}$ is the unit simplex in $\\mathbb{R}^N$.\n",
    "\n",
    "In the following, we consider the initial state of the chain $X_0$, following the discrete probability distribution:\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}(X_0 = k) = \\rho_k, \\qquad k \\in \\{1, \\dots,  N\\}.\n",
    "$$\n",
    "  \n",
    "Let $\\mathbf{A} = [a_{i,j}]_{i,j} \\in \\mathbb{R}^{N \\times N}$ be the transition matrix of the chain, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "    &a_{i,j} = \\mathbb{P}(X_{n+1} = j \\mid X_{n} = i) \\geq 0, \\, \\forall n \\geq 0, \\\\\n",
    "    &(\\forall i \\in \\{1, \\dotsc, N\\}), \\quad \\sum_{j=1}^N a_{i,j} = 1.\n",
    "\\end{align*}\n",
    " \n",
    "The chain is said to be homogeneous in that $\\mathbf{A}$ does not depend from the time index $n$. Let $\\tilde{a}_n$ represent the $n^{th}$ row of $\\mathbf{A}$. \n",
    "\n",
    "The trajectory of the chain can be simulated as follows:\n",
    "\n",
    ">- Draw the discrete random variable $X_0$ with distribution $\\boldsymbol{\\rho}$;\n",
    ">\n",
    ">- For $q = 0$ to $n_{\\text{iter}}-1$\n",
    ">    - Draw the discrete random variable $X_{q+1}$ with distribution $\\tilde{a}_{X_{q}}$;\n",
    ">    \n",
    ">- Return ${(X_q)}_{0 \\leq q \\leq n_{\\text{iter}}}$.\n",
    "\n",
    "\n",
    "<!-- If $X_n = k$, we know that $T$, the life time of the chain in the state $k$ obeys a geometric distribution with parameter $a_{kk}$. We also know that the probability of transition from k to $\\ell\\neq k$ is given by:\n",
    "\n",
    "$$\n",
    "    \\mathbb{P}(X_{n+1}=\\ell | X_n=k, \\ell\\neq k) = \\frac{a_{k\\ell}}{1-a_{kk}}.\n",
    "$$\n",
    "\n",
    " ### One possible algorithm to simulate a Markov chain is therefore:\n",
    "\n",
    "    a. generate the initial state $X_0$ according to the discrete law $\\{\\rho_1,\\dots,\\rho_N\\}$.\n",
    "\n",
    "    b. at instant $n$, knowing that $X_n=k$,\n",
    "\n",
    "    i) determine the life time $T$ in state $X_n=k$ by simulating a geometrical variable with parameter $a_{kk}$. As a consequence $X_n = \\dots = X_{n+T} = k$. When $T=0$, we simply still have $X_n=k$.\n",
    "\n",
    "    ii) determine next transition instant $n+T$, and determine the next state by using the probabilities of transition. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement the above algorithm in a function `X = markov(rho,A,nmax,rng)` with:\n",
    "     - `rho`: law of the initial state (nonnegative vector of size $N$, summing to 1),\n",
    "     - `A`: transition matrix (of size $N\\times N$),\n",
    "     - `nmax`: number of time steps,\n",
    "     - `rng`: random number generator\n",
    "     - `X`: trajectory of the chain.\n",
    "     \n",
    "In particular, check the input parameters `A` and `rho` make sense by adding appropriate assertions (or raising exceptions).\n",
    "\n",
    "> Hint: the function `np.random.choice` can be useful to draw discrete random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_variable(values, rho, nb_points):\n",
    "    plt.figure()\n",
    "    sample = np.random.choice(values, nb_points, p=rho)\n",
    "    side, count = np.unique(sample, return_counts=True)\n",
    "    probs = count / len(sample)\n",
    "\n",
    "    # Plot the results\n",
    "    sns.barplot(x=side, y=probs)\n",
    "    plt.title(\n",
    "        f\"Discrete Probability Distribution ({nb_points} rolls)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xlabel(\"Outcome\")\n",
    "\n",
    "def markov(rho, A, nmax, rng):\n",
    "    X = [rho]\n",
    "    N = rho.shape[0]\n",
    "    niter = 0\n",
    "    \n",
    "    assert A.shape[0] == N and A.shape[1] == N, \"The transition matrix must be squared\"\n",
    "    assert (sum(rho) - 1.0) < 1e-3, \"The components of rho must be summing to one\"\n",
    "    for _ in rho:\n",
    "        assert _ >= 0, \"rho must be a nonnegative vector\"\n",
    "\n",
    "    nb_points = 1000\n",
    "    values = np.linspace(1, N, num=N, dtype=int)\n",
    "\n",
    "    # draw_variable(values, rho, nb_points)\n",
    "\n",
    "    while niter < nmax :\n",
    "        niter+=1\n",
    "        rho = A.T @ rho\n",
    "        X.append(rho)\n",
    "        # draw_variable(values, rho, nb_points)\n",
    "\n",
    "    return X\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set the random number generator to a known state. Make a few simulations using simple transition matrices (*i.e.*, taking any nonnegative matrix $A=(a_{i,j})$ such that its lines sum to 1) and display the trajectory of the chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "rho = np.array([0.05, 0.2, 0.5, 0.2, 0.05])\n",
    "A = np.array([[0.1, 0.2, 0.6, 0.05, 0.05],\n",
    "              [0.2, 0.6, 0.05, 0.05, 0.1],\n",
    "              [0.6, 0.05, 0.05, 0.1, 0.2],\n",
    "              [0.05, 0.1, 0.2, 0.6, 0.05],\n",
    "              [0.1, 0.2, 0.6, 0.05, 0.05]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = markov(rho, A=A, nmax=10, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explore the potential of the [`multiprocessing` package](https://docs.python.org/3/library/multiprocessing.html) to simulate several Markov chains in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hint: the `mutiprocessing.Pool.starmap` or `mutiprocessing.Pool.starmap_async` methods could be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functions import markov\n",
    "\n",
    "rho0_0 = np.array([0.05, 0.2, 0.5, 0.2, 0.05])\n",
    "rho0_1 = np.array([0.1, 0.2, 0.4, 0.2, 0.1])\n",
    "rho0_2 = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "rho0_3 = np.array([0.3, 0.4, 0.1, 0.1, 0.1])\n",
    "rho0_4 = np.array([0.1, 0.3, 0.2, 0.3, 0.1])\n",
    "\n",
    "A = np.array([[0.1, 0.2, 0.6, 0.05, 0.05],\n",
    "              [0.2, 0.6, 0.05, 0.05, 0.1],\n",
    "              [0.6, 0.05, 0.05, 0.1, 0.2],\n",
    "              [0.05, 0.1, 0.2, 0.6, 0.05],\n",
    "              [0.1, 0.2, 0.6, 0.05, 0.05]])\n",
    "nmax = 10\n",
    "rho0 = [rho0_0, rho0_1, rho0_2, rho0_3, rho0_4]\n",
    "\n",
    "\n",
    "with Pool(5) as p:\n",
    "    print(p.starmap(markov, [(rho0[i], A, nmax, rng) for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Bonus] Generate Markov chains in parallel with the [`dask`](https://docs.dask.org/en/latest/futures.html) library, which offers more general parallelization functionalities (with, for instance, the use of [`Futures`](https://docs.dask.org/en/stable/futures.html), see tutorial [here](https://tutorial.dask.org/05_futures.html)). A useful example is provided [here](https://stackoverflow.com/questions/41471248/how-to-efficiently-submit-tasks-with-large-arguments-in-dask-distributed). Note that `dask` is much more versatile and powerful than `multiprocessing`, and can be useful to scale algorithms over multiple cores and/or computing nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from functions import markov\n",
    "\n",
    "params = A, nmax, rng\n",
    "\n",
    "client = Client()  # start local workers as processes\n",
    "# futures = [client.submit(markov, rho_0, param) for param in params]\n",
    "submits = []\n",
    "for i in range(N):\n",
    "    submits.append(client.submit(markov, args[i][0], args[i][1], args[i][2], args[i][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submits = client.gather(submits)\n",
    "submits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"bonus\">Bonus: Parallel computing with Dask</a> [(&#8593;)](#content)\n",
    "\n",
    "1. Take a look at the [`dask.array` documentation](https://docs.dask.org/en/stable/array-best-practices.html) and the associate [tutorial](https://tutorial.dask.org/02_array.html). Apply some of the functions introduced herein and in the [documentation](https://docs.dask.org/en/stable/array-best-practices.html) to parallelize the computation of the total variation investigated during session 2. Note that you can combine `dask` and `numba` to obtain an overall more efficient implementation. Note that timing can be worse than Numpy (`dask.array` is more specifically interesting when the data do no fit in memory).\n",
    "\n",
    "2. Take a look at the [`dask.delayed` tutorial](https://tutorial.dask.org/03_dask.delayed.html), and go through some of the examples provided. [Best practices with the `dask.delayed` interface](https://docs.dask.org/en/stable/delayed-best-practices.html) are summarized in the documentation.\n",
    "\n",
    "> **Remark**: an alternative to Dask: the [Ray](https://docs.ray.io/en/latest/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "3e86c5fcaeb7504a0c486c54f5e7f20bce8324b88f64f392f8b6244d9f0e8929"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
